% ---------------------------------------------------------------------------- %
% OPTIMISATIONS DESCRIPTION
% ---------------------------------------------------------------------------- %
\section{Optimisations} \label{sec:optimisations}
The primary intent of the presentation of the algorithms above was to convey the
idea of Time Forward Processing applied on the topological ordering og OBDDs. We
will now point out multiple avenues to improve the constants in the I/O and time
complexity at the cost of the code complexity.

\subsection{Reduce}
Since \Reduce\ is used at the end of all other algorithms, any small ounce of
speed one can squeeze out of this function will have noticable implications on
the speed of the others. In the presentation of the algorithm above, we already
improved the algorithm by merging the input OBDD with the supporting data
structure of the transposed graph, but we also propose the optimisations below.

\subsubsection{Separate sink-dependency list}
The main bottleneck of the \Reduce\ algorithm is the use of the priority queue
\ReduceQdep, why we would want to minimise the number of elements within. In the
algorithm we assumed \ReduceQdep\ is prepopulated with the sink dependencies and
were able to do do so in all prior algorithms.

We notice, that the sink-dependencies always are placed in \ReduceQdep\ in
topological order, which means all \lstinline{Edges} are sorted with respect to
their \lstinline{source}. Hence, one can instead prepopulate a list
$L_{\mathit{Red:}Sinks}$, from which the sink dependencies are retrieved rather
than from \ReduceQdep.

\todo[inline, caption={Experiment: ratio of sink-arcs}]
{Experimentally settle the ratio of sinks relative to $N$ to discern the
  possible speedup.}

\subsubsection{Early application of the first reduction rule}
At the time of extracting the dependencies \lstinline{e_low} and
\lstinline{e_high} of a node \lstinline{v} from \ReduceQdep, one already has all
the information needed to potentially apply the first reduction rule. Hence, one
may immediately populate the list of rule 1 reduced nodes,
$L_{j:\mathit{red}:1}$ instead of extracting it later in
\lstinline{reduce_layer}. This will save one from having to scan and sort those
elements multiple times.

\todo[inline, caption={Experiment: ratio of rule-1 reduced}]
{Experimentally settle a value the number of rule-1 reduced nodes.}

\subsubsection{Implicit merge of the reduction lists}
\label{sec:optimisations_reduce__merge}

One should notice, that one does not need to concatenate and sort the three
lists $L_{j,\mathit{out}}$, $L_{j,\mathit{red:}1}$, $L_{j,\mathit{red:}2}$ to
create $L_{j:F}$, since one may instead merely sort the three lists and
implicitly take \lstinline{(w,w')} from $L_{j:F}$ by picking the next element
from either of the three lists, similarly to the classic \lstinline{merge}
subprocedure of merge sort.

\subsection{Apply}
Some boolean operators, such as \emph{or}, \emph{and}, and \emph{implication}
all allow for the ability to short-circuit their evaluation. At the start of the
\Apply\ one can evaluate all combinations of the given operator \lstinline{op}
to then conclude what evaluations can be short-circuited. This may decrease the
number of requests in \ApplyQrec\ when sink-nodes become part of the requests.

\todo[inline, caption={Experiment: Efficiency of short-circuit}]
{ Experimentally conclude how many requests and intermediate nodes one can save
  when choosing to short-circuit. }

\subsection{Exists and Relational Product}
Since the \Exists\ and \RelProd\ algorithms work bottom-up, then some of the
optimisations for \Reduce\ also applies to both of these. We especially want to
highlight the merging of lists of results to be forwarded as described above in
Section~\ref{sec:optimisations_reduce__merge}.

One would also notice, that one can postpone the recomputation of \ExistsLres\
and \textbf{??} with the expensive \lstinline{exists_or} and
\lstinline{relprod_or} until a layer depends on requests yet not processed. That
is, if there are no \lstinline{Edges} between any \lstinline{Nodes} in multiple
successive layers, then one may create requests from and reduce all these layers
before sweeping down.

\todo[inline, caption={Experiment: Number of down sweeps}]
{ Experimentally conclude how many layers can be taken in bulk. In practice are
  there always some dependency within the layer? It would seem likely. }

\subsection{Omitting the index} \label{sec:optimisations__no_index}
When one processes a large data set, then one inevitably will do so by
processing a stream. This will then be with an iterator with which one may
\emph{seek} forward or backwards through the stream. Due to the physical nature
of harddisks, almost no time is lost due to this; the readhead still needs to
move, the disk needs to spin, and the branch prediction is able to prefetch a
lot of values.

Since all \lstinline{Nodes} and \lstinline{Edges} are sorted with respect to a
\lstinline{label} and \lstinline{id}, then the index itself is not needed to
identify the needed value. Hence, to save space, we may choose to completely
omit the index and instead only rely on the ordering.

\subsection{Representation of nodes for faster sorting}
Inspired by \cite{Dijk16} we propose to represent the \lstinline{NodeArc} type
of Code~\ref{lst:data_node} as a single 64-bit integer as shown in
Figure~\ref{fig:data}. A \lstinline{NodeArc.Sink} of value $v \in \{0,1\}$ is
represented by a $1$-flag on the most significant bit, $v$ on the least
significant bit, and all other bits set to $0$.\footnote{Notice, that we as such
  leave room for sinks taking on non-boolean values of up to $2^{63}$ bits.} A
\lstinline{NodeArc.Ptr} has a $0$-flag on the most significant bit, the next
$k$-bits dedicated to the \lstinline{label}, and finally the $63-k$ least
significant bits contain the \lstinline{id}.

A \lstinline{Node} can then be represented by $3$ $64$-bit numbers:
Two $64$-bit integers for each child and another $64$ bits for the
\lstinline{label} and \lstinline{index} of the node itself with the same layout
as for \lstinline{NodeArc.Ptr}.

\begin{figure}[ht!]
  \centering

  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_sink.tex}
    \caption{\lstinline{NodeArc.Sink\{value: v\}} where \lstinline{v} $\in
      \{0,1\}$}
    \label{fig:data_sink}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_link.tex}
    \caption{\lstinline{NodeArc.Ptr\{label, index\}}}
    \label{fig:data_link}
  \end{subfigure}

  \caption{Visual representation of data layout. The least significant bit is
    right-most.}
  \label{fig:data}
\end{figure}

Reexamining the algorithms above, one will notice that only the combination of
\lstinline{label} and \lstinline{id} has to be unique, not \lstinline{id} by
itself. Hence, \lstinline{id} may be reset for each layer, thereby enlarging the
possible size of each layer and with it the total number of nodes in the graph.
A reasonable value for $k$ is $28$, since this supports OBDDs of up to $2.6
\cdot 10^8$ variables, $3.4 \cdot 10^{10}$ nodes per layer for a total
$2^{63}$ nodes in the OBDD.

One should notice, that the sorting in the previous section with this layout in
a single number is now reduced to a mere trivial sorting on $64$-bit numbers. In
the case of the bottom-up sweep algorithm, the sorting is in descending order,
while the top-down sweep algorithms is in ascending order. Tuples $(t_1,t_2)$ in
\Apply\ and \Isomorphic\ are then $64$-bit numbers $\min(t_1,t_2)$ when
\lstinline{data} is not present. Since \lstinline{data} is only present for
pairs of nodes with the same \lstinline{label}, then the number to sort by
simply becomes $\max(t_1,t_2)$. Whether it is faster to repeatedly compute
$\min$ and $\max$ during sorting, or precompute it at insertion time at the cost
of increasing the size of each element by another $64$-bits, is worth
experimental investigation.

\todo[inline, caption={Experiment: Precompute sorting number}]
{Experimentally settle whether one should precompute for the sorting algorithm
  or instead save space usage.}

The representation of variables \lstinline{Node} and \lstinline{NodeArc} as a
single $64$-bit integers does result in having to do bit shifts and bit masking
to access the values. Such instructions though are computationally very cheap,
and are quickly offset by the smaller memory size and especially the much
cheaper comparisons during sorting. This reduction to numbers might even open up
investigating the use of $O(N)$ time and $O(\sort(N))$ I/O non-comparative sorting
algorithm, such as the ones in \todocite.

\subsection{Equality Checking}
One should notice, that all algorithms as they have been described above are
\emph{stable} in the sense, that the ordering of nodes within each layer is
preserved. That means, one can check whether an OBDD $G$ is unchanged after one
or more manipulations by comparing it with the result $G'$ in a single linear
scan, which only takes $O(N/B)$ I/Os.

\subsubsection{Taint Tracking}
The primary use case for equality checking is in model checking, where one wants
to compute the transitive closure by repeatedly applying \RelProd\ until the set
of states are unchanged. The memoisation table approach is able to do so with an
equality check with in $O(1)$ time by merely comparing pointers. If one wants to
achieve the same in the algorithms above, then one needs to deploy the use of
taint tracking.

For the \Exists\ and \RelProd\ one can decrease this all the way down to an
$O(1)$ operation by introducing taint tracking in \lstinline{Nodes} and
\lstinline{Edges}: \lstinline{False} if it is an original node and
\lstinline{True} otherwise. On a top-down sweep any newly generated node will be
tainted with \lstinline{True} while original nodes are marked \lstinline{False}.
The taint tracking is propagated bottom-up during the \Reduce\ by a disjunction
of the taint label on its two children.

While the \Reduce\ already favours keeping the original node, one should ensure
this definitely is the case. The \lstinline{exists_or} and
\lstinline{relprod_or} algorithms simultaneously cannot choose to garbage
collect any original nodes early by the off-chance, that they might get
reintroduced later. Instead, the \lstinline{exists_or} will keep all the
original nodes up until the very last quantification downward sweep, and
similarly the \lstinline{relprod_or} only discards the original dead nodes of
\lstinline{S} when processing the root layer. This adds up to another
$O(sort(N^{n'}))$ amount of work, which does not change their asymptotic running
time or I/O bound.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% ispell-local-dictionary: "british"
%%% End:
